# Introduction
In the healthcare domain, effective communication between doctors and patients is crucial for accurate diagnosis and treatment. However, documenting these conversations and summarizing them into actionable clinical notes can be time-consuming. Leveraging large language models (LLMs) like Llama2 for this purpose offers the potential to streamline this process, improving both the quality and efficiency of healthcare delivery.
Statement of the Problem:
Existing LLMs are not specifically trained for the healthcare domain, leading to suboptimal performance when generating or summarizing medical dialogues. There is a need to fine-tune these models using domain-specific datasets to enhance their relevance and accuracy in medical contexts.

# Objectives:
-	To fine-tune a pre-trained LLM (Llama2) on a healthcare-specific dataset.
-	To evaluate the model's performance in generating and summarizing medical dialogues.
-	To deploy a resource-efficient model that can be used in real-world healthcare applications. 

# Data Collection and Preprocessing

## Description of the Data Sources:
The dataset used for this project is NoteChat(https://huggingface.co/datasets/akemiH/NoteChat) , sourced from Hugging Face. It consists of approximately 207,000 rows, each containing a dialogue between a doctor and a patient (or the patient's family) and a corresponding summary. Each conversation in the dataset averages around 600 tokens, while the summarization for each row typically averages around 250 tokens. These averages may fluctuate slightly based on data shuffling, but generally remain close to these figures.

## Data Preprocessing Steps:
-	Cleaning: Removal of any irrelevant or noisy data, such as incomplete dialogues or incorrect summaries. Removed extra space at the end. 
-	Formatting: Based on the LLM requirements, conversations are formatted as follows:
  Conversation Format: ### Patient: Hello ### Assistant: Hi, how can I help you? These formatted conversations are placed in dedicated columns.
 	Summary Format: Summaries are generated by condensing the entire conversation and the assistant's replies into a concise summary.
-	Transformation: Tokenization using Auto Tokenizer from the pre-trained Llama2 model, with padding applied to the right side.
-	Feature Engineering: Extracted medical tokens and adding to prompt to reduce hallucination.

## Challenges and Solutions:
#### Challenge: Balancing the context length in conversations.
-	Explanation: During testing on the GPU, we encountered an issue where the trained model's output included both replies and user prompts, which increased the overall token count. This required setting an appropriate token limit during conversations.
-	Solution: Adjusted the token limits for conversations to ensure the model's responses remained within the desired token range. While the minimum token count was supposed to be 30, the model occasionally returned more. To address this, we experimented with the maximum token limit, influencing the CPU to adjust the model's responses effectively, ultimately setting it around 50 tokens.
#### Challenge: Dealing with medical jargon.
-	Solution: We fine-tuned the tokenizer to handle domain-specific terminology, ensuring accurate processing of medical language. Additionally, we selected doctor and patient conversations from the NoteChat dataset for fine-tuning both the conversation and summarizer models.
#### Challenge: Computing and Storage Resources
-	Challenges with Compute Resources:
Explanation: Managing the compute resources was challenging due to the high demands of training a large language model like Llama2. The need for sufficient GPU memory, especially when handling large datasets and complex fine-tuning processes, posed significant constraints.
Solution: We optimized the model training process using PEFT techniques (QLoRA and Bits and Bytes) to reduce the computational load. Additionally, model quantization was employed to decrease the overall model size, making it more manageable for deployment on limited hardware.
-	Challenges with Storage Resources:
Explanation: The storage of large datasets and models required efficient management. The base model, adapters, and the final quantized model all demanded significant storage capacity. Saving and took very long time in GitHub, including the time we tried GITLFs.
-	Solution: Hugging face hub provides 300 Gb of storage which supported directly saving the model in git using the hub API.

 ![Ollama](https://github.com/user-attachments/assets/79515f26-4c6f-4532-b6a6-ea9f859e3592)

 
# Methodology

## Machine Learning Algorithms and Techniques:
-	Model: Llama2, chosen for its strong performance in language tasks and manageable size (7 billion parameters).
-	Dataset Preparation: The NoteChat had both the conversation and its summary as it’s column. We had to format both features before training.
-	Model Selection: A pre-trained base model (Llama2) is chosen.
-	PEFT Techniques:
-	Bits and Bytes: Optimizes GPU usage during training.
-	QLoRA: Applied to make the fine-tuning process more efficient.
-	Adapter Model: Created from the dataset and fine-tuned on Llama2.
-	Merging: The adapter model is merged with the base model.
-	Quantization: The merged model undergoes 4-bit quantization for efficient CPU inference.
-	Deployment: The final model is deployed to the Hugging Face Hub.
-	Output: Generates dialogues or summaries based on selected models, optimized for CPU performance.

## Algorithm Choice:
Llama2 was selected due to its balance of performance and resource efficiency and regular API response, making it suitable for fine-tuning on domain-specific tasks like medical dialogue generation and summarization.
Model Training, Validation, and Evaluation:
-	Training: Conducted on a GPU with less than 15GB of memory, utilizing QLoRA for efficient fine-tuning.
-	Validation: Used a subset of the dataset to monitor the model's performance and adjust parameters.
-	Evaluation: Performance was measured using the ROUGE metric, focusing on the quality of the summaries generated.

## Parameter Tuning and Optimization:
-	Parameter Optimization: Focused on optimizing the learning rate and batch size to improve model convergence. After one epoch there was no difference between the training loss between both the models. 
-	Quantization: The final model was 4-bit quantized to reduce its size, making it more efficient for CPU inference.

![2](https://github.com/user-attachments/assets/ec670895-8fed-4b61-b6ff-65cfd3338863)

 
# Results

## Experimental Results:
-	The fine-tuned Llama2 model demonstrated significant improvements in generating coherent medical dialogues and accurate summaries compared to its base model. The fine-tuned model without explicitly told, it interacted with the user similarly as the training dataset, unlike the prompt engineered pre-trained model.
Performance Metrics:
-	ROUGE Score: Used to assess the quality of the summaries. The model achieved a high ROUGE score, indicating strong summarization capabilities.

![image](https://github.com/user-attachments/assets/bfe203c7-b92f-45af-9361-c7ba2181a472)

Our model root score is based on the output generated from base model and the fined tuned model.

## Comparison of Models:
-	The fine-tuned Llama2 model outperformed its base version for summarization and in par in term of conversational fined tuned model trained in NoteChat dataset.
Montoring Training and Tenser Board:
-	During the training we shared the report with the tensor board to track the training process. Tensor Board was utilized to monitor the training process, providing visual insights into model performance metrics such as loss, accuracy, and gradient distributions. This tool helped in identifying potential issues and making real-time adjustments during the fine-tuning process.

![image](https://github.com/user-attachments/assets/3da26466-c7f8-4ce8-9b7e-e91755667e4d)

 
# Discussion

## Interpretation of Results:
The results demonstrate that fine-tuning Llama2 using QLoRA and Bits and Bytes methods leads to a highly efficient model with only 0.95% of its parameters being trainable. This fine-tuning approach, combined with model quantization, allows the model to perform well on dialogue generation and clinical note summarization tasks while being resource efficient. The model's performance metrics, including ROUGE scores, indicate that it is well-suited for deployment in clinical applications and can be tested further by increasing the training examples and epochs. However, further experimentation with larger datasets and different evaluation metrics could potentially enhance the model's robustness and accuracy.

## Strengths and Weaknesses:
-	Strengths: High-quality dialogue generation without prompt engineering, efficient use of resources, and strong summarization capabilities. Leveraging LLM with billions of parameters with training.
-	Weaknesses: The model may struggle with extremely complex medical cases not well represented in the training data. The model maybe behaves like the training data when not necessary, which is called the bias. The model may go out of topic which may generate random tokens which is termed hallucinations. 

## Unexpected Outcomes:
The model's performance on rare medical conditions was lower than expected, likely due to the limited representation of such cases in the dataset.
Comparison with Prior Work:
Compared to previous attempts at fine-tuning LLMs for healthcare, this project achieved higher performance metrics and produced a more resource-efficient model that could be used on local machine with CPU or GPU, contributing valuable insights to the domain.

## Test Cases

### Test Case Description:
The model was tested using a variety of scenarios, including typical doctor-patient conversations, complex medical cases, and dialogues involving family members.
Rationale for Test Case Selection:
Test cases were chosen from random rows that were not part of the training, to cover a broad range of medical scenarios to ensure the model's robustness and reliability in diverse real-world situations.

## Test Results:
-	Success Rate: The model was evaluated from two approaches, intrinsic and extrinsic. For intrinsic we used human to interact with the bot to evaluated how the model is performing. For extrinsic, includes using the rows to evaluate the model with unigram, bigram and more.
-	Issue Resolution: Any issues encountered were addressed by adjusting the model's parameters, changing the prompt and experimenting with system instructions as well as customizing model files, in this case Prompt engineering from Ollama.

### Test Environment and Tools:
Testing was conducted in a controlled environment using Python-based tools and frameworks and Google Colab and Kaggle were used as its runtime choice for GPU and Streamlit for UI testing and evaluation.

### Coverage of Scenarios:
The test cases covered edge cases, such as rare medical conditions, and typical use cases, like routine doctor-patient interactions, ensuring the model's comprehensive reliability.

![image](https://github.com/user-attachments/assets/ea6f3fc4-708a-4612-a9b4-1207ce5e437e)

![image](https://github.com/user-attachments/assets/ad42bc7b-159a-478c-ab8e-ce3164643643)


 
# Conclusion
The project successfully fine-tuned a Llama2 model for dialogue generation and clinical note summarization using a custom dataset. The model was efficiently trained using QLoRA and Bits and Bytes, with only 0.95% of its parameters being trainable. The final 4-bit quantized model in GGUF format demonstrated strong performance while being significantly smaller in size. The final loss were, 0.963100 and 1.102800 that was better compared to models like Facebook opt which was reduced to 350% than the original model.

# References
- NoteChat Dataset: https://huggingface.co/datasets/akemiH/NoteChat 
- Llama2 Model: Hugging Face Repository https://huggingface.co/docs/transformers/en/model_doc/llama2 
#### Final Quantization Tool: https://huggingface.co/spaces/ggml-org/gguf-my-repo 
#### Base Model: https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b 
#### Model Repo:
- https://huggingface.co/atomiCode/peft_llama_chatbot-Q4_K_M-GGUF 
- https://huggingface.co/atomiCode/peft_llama_chatbot-Q4_K_M-GGUF 

